alertmanager:
  enabled: true
  ingress:
    enabled: true
    ingressClassName: traefik
    annotations:
      traefik.ingress.kubernetes.io/router.middlewares: traefik-redirect@kubernetescrd
    hosts:
      - alertmanager.gavriliu.com
  config:
    route:
      group_by: ['...']
      group_wait: 1m
      group_interval: 30s
      receiver: Default
      routes:
      - receiver: "alertmanager-to-github"
    receivers:
      - name: Default
      - name: "alertmanager-to-github"
        webhook_configs:
          - url: "http://alertmanager-to-github.alertmanager-to-github.svc.cluster.local/v1/webhook?owner=AndreiGavriliu&repo=homelab"

defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: true
    configReloaders: true
    general: true
    k8sContainerCpuUsageSecondsTotal: true
    k8sContainerMemoryCache: true
    k8sContainerMemoryRss: true
    k8sContainerMemorySwap: true
    k8sContainerResource: true
    k8sContainerMemoryWorkingSetBytes: true
    k8sPodOwner: true
    kubeApiserverAvailability: true
    kubeApiserverBurnrate: true
    kubeApiserverHistogram: true
    kubeApiserverSlos: true
    kubeControllerManager: false
    kubelet: true
    kubeProxy: false
    kubePrometheusGeneral: true
    kubePrometheusNodeRecording: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    kubeSchedulerAlerting: false
    kubeSchedulerRecording: false
    kubeStateMetrics: true
    network: true
    node: true
    nodeExporterAlerting: true
    nodeExporterRecording: true
    prometheus: true
    prometheusOperator: true
    windows: true

additionalPrometheusRulesMap:
  rule-name:
    groups:
    - name: longhorn.rules
      rules:
      - alert: LonghornVolumeActualSpaceUsedWarning
        annotations:
          description: The actual space used by Longhorn volume {{$labels.volume}} on {{$labels.node}} is at {{$value}}% capacity for
            more than 5 minutes.
          summary: The actual used space of Longhorn volume is over 90% of the capacity.
        expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 90
        for: 5m
        labels:
          issue: The actual used space of Longhorn volume {{$labels.volume}} on {{$labels.node}} is high.
          severity: warning
      - alert: LonghornVolumeStatusCritical
        annotations:
          description: Longhorn volume {{$labels.volume}} on {{$labels.node}} is Fault for
            more than 2 minutes.
          summary: Longhorn volume {{$labels.volume}} is Fault
        expr: longhorn_volume_robustness == 3
        for: 5m
        labels:
          issue: Longhorn volume {{$labels.volume}} is Fault.
          severity: critical
      - alert: LonghornVolumeStatusWarning
        annotations:
          description: Longhorn volume {{$labels.volume}} on {{$labels.node}} is Degraded for
            more than 10 minutes.
          summary: Longhorn volume {{$labels.volume}} is Degraded
        expr: longhorn_volume_robustness == 2
        for: 10m
        labels:
          issue: Longhorn volume {{$labels.volume}} is Degraded.
          severity: warning
      - alert: LonghornNodeStorageWarning
        annotations:
          description: The used storage of node {{$labels.node}} is at {{$value}}% capacity for
            more than 5 minutes.
          summary:  The used storage of node is over 70% of the capacity.
        expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 70
        for: 5m
        labels:
          issue: The used storage of node {{$labels.node}} is high.
          severity: warning
      - alert: LonghornDiskStorageWarning
        annotations:
          description: The used storage of disk {{$labels.disk}} on node {{$labels.node}} is at {{$value}}% capacity for
            more than 5 minutes.
          summary:  The used storage of disk is over 70% of the capacity.
        expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 70
        for: 5m
        labels:
          issue: The used storage of disk {{$labels.disk}} on node {{$labels.node}} is high.
          severity: warning
      - alert: LonghornNodeDown
        annotations:
          description: There are {{$value}} Longhorn nodes which have been offline for more than 5 minutes.
          summary: Longhorn nodes is offline
        expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status{condition="ready"} == 1) or on() vector(0)) > 0
        for: 5m
        labels:
          issue: There are {{$value}} Longhorn nodes are offline
          severity: critical
      - alert: LonghornInstanceManagerCPUUsageWarning
        annotations:
          description: Longhorn instance manager {{$labels.instance_manager}} on {{$labels.node}} has CPU Usage / CPU request is {{$value}}% for
            more than 5 minutes.
          summary: Longhorn instance manager {{$labels.instance_manager}} on {{$labels.node}} has CPU Usage / CPU request is over 300%.
        expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 > 300
        for: 5m
        labels:
          issue: Longhorn instance manager {{$labels.instance_manager}} on {{$labels.node}} consumes 3 times the CPU request.
          severity: warning
      - alert: LonghornNodeCPUUsageWarning
        annotations:
          description: Longhorn node {{$labels.node}} has CPU Usage / CPU capacity is {{$value}}% for
            more than 5 minutes.
          summary: Longhorn node {{$labels.node}} experiences high CPU pressure for more than 5m.
        expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 > 90
        for: 5m
        labels:
          issue: Longhorn node {{$labels.node}} experiences high CPU pressure.
          severity: warning
    - name: kasten.rules
      rules:
      - alert: K10BackupFailed
        annotations:
          description: K10 backup {{$labels.backup}} failed.
          summary: K10 backup {{$labels.backup}} failed.
        expr: k10_backup_status{status="failed"} > 0
        for: 5m
        labels:
          issue: K10 backup {{$labels.backup}} failed.
          severity: critical
      - alert: K10BackupDuration
        annotations:
          description: K10 backup {{$labels.backup}} took {{$value}} seconds.
          summary: K10 backup {{$labels.backup}} took more than 2 hour.
        expr: k10_backup_duration_seconds > 7200
        for: 5m
        labels:
          issue: K10 backup {{$labels.backup}} took more than 1 hour.
          severity: warning
    - "name": "argo-cd"
      "rules":
      - "alert": "ArgoCdAppSyncFailed"
        "annotations":
          "dashboard_url": "https://grafana.com/d/argo-cd-application-overview-kask/argocd-application-overview?var-dest_server={{ $labels.dest_server }}&var-project={{ $labels.project }}&var-application={{ $labels.name }}"
          "description": "The application {{ $labels.dest_server }}/{{ $labels.project }}/{{ $labels.name }} has failed to sync with the status {{ $labels.phase }} the past 10m."
          "summary": "An ArgoCD Application has Failed to Sync."
        "expr": |
          sum(
            round(
              increase(
                argocd_app_sync_total{
                  job=~".*",
                  phase!="Succeeded"
                }[10m]
              )
            )
          ) by (cluster, job, dest_server, project, name, phase) > 0
        "for": "1m"
        "labels":
          "severity": "warning"
      - "alert": "ArgoCdAppUnhealthy"
        "annotations":
          "dashboard_url": "https://grafana.com/d/argo-cd-application-overview-kask/argocd-application-overview?var-dest_server={{ $labels.dest_server }}&var-project={{ $labels.project }}&var-application={{ $labels.name }}"
          "description": "The application {{ $labels.dest_server }}/{{ $labels.project }}/{{ $labels.name }} is unhealthy with the health status {{ $labels.health_status }} for the past 15m."
          "summary": "An ArgoCD Application is Unhealthy."
        "expr": |
          sum(
            argocd_app_info{
              job=~".*",
              health_status!~"Healthy|Progressing"
            }
          ) by (cluster, job, dest_server, project, name, health_status)
          > 0
        "for": "15m"
        "labels":
          "severity": "warning"
      - "alert": "ArgoCdAppOutOfSync"
        "annotations":
          "dashboard_url": "https://grafana.com/d/argo-cd-application-overview-kask/argocd-application-overview?var-dest_server={{ $labels.dest_server }}&var-project={{ $labels.project }}&var-application={{ $labels.name }}"
          "description": "The application {{ $labels.dest_server }}/{{ $labels.project }}/{{ $labels.name }} is out of sync with the sync status {{ $labels.sync_status }} for the past 15m."
          "summary": "An ArgoCD Application is Out Of Sync."
        "expr": |
          sum(
            argocd_app_info{
              job=~".*",
              sync_status!="Synced"
            }
          ) by (cluster, job, dest_server, project, name, sync_status)
          > 0
        "for": "15m"
        "labels":
          "severity": "warning"
      - "alert": "ArgoCdAppAutoSyncDisabled"
        "annotations":
          "dashboard_url": "https://grafana.com/d/argo-cd-application-overview-kask/argocd-application-overview?var-dest_server={{ $labels.dest_server }}&var-project={{ $labels.project }}&var-application={{ $labels.name }}"
          "description": "The application {{ $labels.dest_server }}/{{ $labels.project }}/{{ $labels.name }} has autosync disabled for the past 2h."
          "summary": "An ArgoCD Application has AutoSync Disabled."
        "expr": |
          sum(
            argocd_app_info{
              job=~".*",
              autosync_enabled!="true",
              name!~""
            }
          ) by (cluster, job, dest_server, project, name, autosync_enabled)
          > 0
        "for": "2h"
        "labels":
          "severity": "warning"
      - "alert": "ArgoCdNotificationDeliveryFailed"
        "annotations":
          "dashboard_url": "https://grafana.com/d/argo-cd-notifications-overview-kask/argocd-notifications-overview?var-job={{ $labels.job }}&var-exported_service={{ $labels.exported_service }}"
          "description": "The notification job {{ $labels.job }} has failed to deliver to {{ $labels.exported_service }} for the past 10m."
          "summary": "ArgoCD Notification Delivery Failed."
        "expr": |
          sum(
            round(
              increase(
                argocd_notifications_deliveries_total{
                  job=~".*",
                  succeeded!="true"
                }[10m]
              )
            )
          ) by (cluster, job, exported_service, succeeded) > 0
        "for": "1m"
        "labels":
          "severity": "warning"
    - name: blackbox-exporter.rules
      rules:
      - alert: IngressDown
        expr: probe_success == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Ingress {{ $labels.instance }} is unreachable"
      - alert: CertExpiringSoon
        expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 7
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "TLS cert for {{ $labels.instance }} expires in < 7 days"
    - name: oomkilled.rules
      rules:
      - alert: PodOOMKilled
        expr: |
          increase(kube_pod_container_status_terminated_reason{reason="OOMKilled"}[5m]) > 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Pod OOMKilled in {{ $labels.namespace }}/{{ $labels.pod }}"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} was OOMKilled in the last 5 minutes."
global:
  rbac:
    create: true

grafana:
  enabled: false

prometheus:
  ingress:
    enabled: true
    ingressClassName: traefik
    annotations:
      traefik.ingress.kubernetes.io/router.middlewares: traefik-redirect@kubernetescrd
    hosts: 
      - prometheus.gavriliu.com
  prometheusSpec:
    ruleSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteMany"]
          resources:
            requests:
              storage: 60Gi
    additionalScrapeConfigs:
    - job_name: "pi-hole"
      static_configs:
          - targets: ["prometheus-pihole-exporter.kube-prometheus-stack.svc:9100"]
    - job_name: 'node-exporter'
      static_configs:
        - targets: ['10.0.0.5:9100', '10.0.0.10:9100', '10.0.0.30:9100', '10.0.0.101:9100']
    - job_name: k10
      scrape_interval: 15s
      honor_labels: true
      scheme: http
      metrics_path: '/k10/prometheus/federate'
      # params:
      #   'match[]':
      #     - '{__name__=~"jobs.*"}'
      static_configs:
        - targets:
          - 'prometheus-server.kasten-io.svc.cluster.local'
          labels:
            app: "k10"
    - job_name: ingress-blackbox
      metrics_path: /probe
      params:
        module: [http_2xx]
      kubernetes_sd_configs:
        - role: ingress
      relabel_configs:
        - source_labels: [__meta_kubernetes_ingress_host]
          target_label: __param_target
        - source_labels: [__param_target]
          target_label: instance
        - target_label: __address__
          replacement: blackbox-exporter-prometheus-blackbox-exporter.kube-prometheus-stack.svc.cluster.local:9115
